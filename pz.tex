\documentclass[a4paper,english,russian]{G2-105}
\usepackage[T1]{fontenc}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{standalone}
\usepackage{newclude}
\usepackage[final]{pdfpages}
\usepackage{multirow}
\usepackage{sansmath} % Enables turning on sans-serif math mode, and using other environments
\sansmath % Enable sans-serif math for rest of document
\usepackage[math]{blindtext}

%\usepackage[utf8]{luainputenc}
\VSTUSetDocumentNumbersPrefix{}
\VSTUSetDocumentCode{ВСТАВИТЬ КОД}
\VSTUSetDocumentTypeDative{выпускной работе бакалавра}
\VSTUSetDocumentTypeGenitive{выпускную работу бакалавра}
\VSTUSetInitialData{задание, выданное научным руководителем с кафедры САПР и ПК,
утвержденное приказом ректора}

\begin{document}
\VSTUSetOrder{????–ст}{??}{??????}{201?}
\VSTUSetFaculty{Электроники и вычислительной техники}
\VSTUSetDepartment{Системы автоматизированного проектирования и ПК}
\VSTUSetDepartmentCode{??.??}
\VSTUSetDirection{??.??.?? Автоматизированные системы управления}
\VSTUSetHeadOfDepartment{Зав. кафедрой САПР и ПК}{д.т.н., ??.}{М. В. Щербаков}{Щербаков Максим Владимирович}
\VSTUSetDirector{старший преподаватель САПР и ПК}{}{А. В. Катаев}{Катаев Александр Вадимович}
\VSTUSetFacilityExpert{}{}{}{}
\VSTUSetStandardsAdviser{????}{?????}{???????????}{?????? ?????? ????????????}
\VSTUSetStudent{ИВТ-461}{Т. А. Мельников}{Мельников Тимофей Алексеевич}
\VSTUSetTitle{Портирование сверточной нейросети на ARM архитектуру с ограниченными вычислительными ресурсами и ресурсами памяти}
\VSTUSetTitleEng{Porting a convolutional neural network on an ARM architecture, taking into account the computing resources and memory resources}
%\VSTUAddChapterWordToTOC % обязательно для ПЗ в магистерских диссертациях
\VSTUInitializePZ
\abstract{Аннотация}
\par Документ представляет собой пояснительную записку к выпускной работе бакалавра на тему «Портирование сверточной нейросети на ARM архитектуру с ограниченными вычислительными ресурсами и ресурсами памяти», выполненную студентом группы ИВТ-461, Мельниковым Тимофеем Алексеевичем.
\par В данной работе рассмотрена возможность реализации алгоритмов машинного обучения, в частности прямой проход сверточной нейронной сети, на устройстве с ограниченными вычислительными ресурсами и ресурсами памяти.
\par Объём пояснительной записки составил \totalpages~страниц и включает \totalfigures~рисунков и \totaltables~таблицы. 

%\newpage
\tableofcontents
\newpage

\starchapter{Введение}
\par Задачи обработки и анализа аналоговой информации являюся одиними из самых сложных в IT-индустрии. Долгое
время такие задачи решались евристическими алгоритмами, которые требовали огромных аппаратных ресурсов при малой точности результата. На протяжении последних десяти лет стремительно растет и развивается прикладная область математики цель которой, изучение и развитие искусственных нейронных сетей. Актуальность разработок и исследований в данной области оправдывается применением НС в различных сферах деятельности. Это автоматизация процессов анализа объектов, образов, уневерсализация управления, прогнозирование, создание экспертных систем, анализ неформализованной информации и многое другое. В частности, в данной дипломной работе используются нейронные сети для классификации и обнаружения объектов на изображении. 
\par Наиболее существенным недостатком НС является их требовательность к вычислительным ресурсам и ресурсам памяти. Частично данная проблема решается использованием сверточных нейронных сетей, которые, в виду особенностям логики работы, позволяют в разы сократить ресурсы потребляемые нейронной сетью.
\par Однако, не только искусственные нейронные сети являются трендом IT-идустрии, активно развивается коцепция интернета вещей. Диапазон встраиваемых технологий простирается от концепции умных зданий до промышленной консолидации. Совмещение встраиваемых систем и искусственных нейронных сетей позволяет иначе взглянуть на решение нетревиальных задачь, таких как автономное управление автомобилем.
\par В связи с вышесказанным целью данной дипломной работы является внедрение фреймворка машинного обучения на embedded систему C.H.I.P. и последующая оптимизация его работы. На основе проделанной работы необходимо сделать вывод о эффективности и рентабельности данного решения. 
\par Для достижения поставленной цели необходимо решить следующие задачи:
\begin{itemize}
\item Изучить фреймворки глубокого машинного обучения
\item Разработать консольное приложение для реализации прямого прохода нейронной сети
\item Оптимизировать использование оперативной памяти и реализовать загрузку весов по мере использования
\item Разработать клиент-серверное приложение, демонстрирующее результат работы
\end{itemize}
\par В первом разделе пояснительной записки описаны фрейворки машинного обучения. Далее приведено обоснование выбора фреймворка darknet.
\par Во втором разделе описаны используемые модели нейронных сетей и алгоритм прямого прохода.
\par Третей раздел посвящен разворачиванию фреймворка на устройстве C.H.I.P. и оптимизации работы алгоритма прямого прохода. Так же описана разработка клиент-серверной части для визуализации работы приложения. 
\newpage

\chapter{Обзор фреймворков машинного обучения}
\par Данные раздел содержит справочную информацию, технические особенности и функциональные возможноти фреймворков глубоко машинного обучения и их сравнение. Раздел содержит обоснование выбора фреймворка Darknet для встраивания и оптимизации на мобильном пк C.H.I.P.
\par Из всего множества фрейворков были выделены Caffe, Torch7, Darknet, как наиболее зрелыe, функционально полныe и широко используемыe.
\section{Caffe}
\par Caffe представляет собой фреймворк, разработанный учеными и практиками, с прозрачной и гибкой архитектурой для глубокого обучения и построения эталонных моделей. Фреймворк распространяется под BSD-лицензией и является c++ библиотекой. Так же реализованы python и MATLAB обертки для универсализации обучения и развертывания глубоких моделей. Caffe используется на промышленных компаниях и в медиацинтрах, обрабатывая 40 миллионов изображений в день на Titan GPU (примерно 2.5 милисекунд на изображение).
\par Caffe поддерживается и разрабатывается университетом Беркли, а именно центром BVLC.
\subsection{Основные характеристики Caffe}
\par Caffe представляет полный набор инструментов для обучения, тестирования, настройки и разработки моделей с подробной документацией и примерами. Поэтому обучиться использовать фреймворк можно довольно быстро. Возможность использования GPU делает Caffe одним из самых быстрых фреймворков, что позволяет его использовать в промышленном секторе. Такие показатели достигнуты благодаря особенностям описаным ниже.
\par Caffe является модульным программным обеспечением. Что позволяет легко добалять новые форматы данных, слои и функции потерь. В фреймворке уже реализовано множество слоев и функций потерь, что позволяет реалзовавать нейронную сеть для задачь различных предметных областей и категорий.
\par В Caffe представление и реализация разделены. Для описания модели в Caffe используется конфигурационный файл в формате protobuf. Caffe поддерживает сетевые архитектуры в форме произвольно ориентированных ациклических графов. Важной деталей является то, что после создания экземпляра модели Caffe выделяется ровно столько памяти, сколько необходимо для работы сериализованной нейронной сети и для хранения адреса объекта [1].
\par В Caffe используется полное тестовое покрытие. Каждый модуль имеет собственный набор тестов. Модуль будет принят, только после прохождение всего набора тестов. Это позволяет эффективно оптимизировать модули и гарантирует стабильную работу фреймворка.
\par Caffe содержит предворительно обученные модели для академических целей и некоммерческого использования. Доступны сверточные НС с архитектурой "AlexNet" и вариации данной НС, обученные на базе данных ImageNet[2]. Так же доступны реккурентные модели[3].
\subsection{Архитектура Caffe}
\par Caffe сохраняет и передает данные в четырехмерных массивах, которые названы блобами. Блобы представляют унифицированный интерфейс для работы  памятью, содержащий пакеты ихображений (или других данных), параметров или обновлений параметров. Блобы скрывают вычислительные издержки смешанной работы CPU и GPU, выполняя синхронихацию по нере необходимости. Память выделяется по требованию (лениво), что позволяет эффективней ее использовать. Модели сохраняются как буфер, использующий протокол Google (Google Protocol Buffers), который имеет ряд достоинств: минимальный размер строки при сериализации, эффективная сериализация, высокая читабельность в текстовом виде и удобные интерфейсы работы на нескольких языках. Необходимые для обучения огромные массивы данных храняться в базах данныx LevelDB. Google Protocol Buffers и LevelDB обеспечивают пропускную способность в 150 Мб/с. 
\par Слой в Caffe представляет собой структуру соответствующую формальному определению слоя: он принимает на вход один или несколько блобов и выдает один или несколько блобов результатом. Caffe предоставляет полный набор типов слоев для глубокого обучения, включая сверточный, pooling слой, inner products слой, нелиности, такие как выпремленная линейная и логическая, слои потерь, таких как softmax и hinge. Настройка слоя требует минимальных усилий в виду композиционного построения сетей.
\par Caffe обеспечивает функциональность для любого направленного ациклического графа слоев, позволяя корректно выполнять прямой и обратный проход. Модели Caffe --- это сквозные системы машинного обучения.[1]
\subsection{Приемущества Caffe}
\par От других современных фреймворков глубокого обучения Caffe отличается следующими качествами:
\begin{itemize}
	\item Реализция полностью основана на C++, что облегчает интеграцию с встраиваемыми системами. CPU режим позволяет использовать фреймворк без специализированного GPU.
	\item Готовые модели позволяют не тратить время и ресурсы на обучение. Важным пунктом является подробная документация для сериализации и использования моделей.
\end{itemize}

\section{Torch7}
\par Torch7 --- это универсальный математический фреймворк и библиотека машинного обучения, которая имеет оболочку для языка программирования Lua. Его цель --- предоставить гибкую среду для проектирования и обучения моделей глубокого обучения. Гибкость достигается с помощью Lua, так как он является очень легким скриптовым языком. Эффективная реализация низкоуровневых числовых процедур, используя OpenMP и CUDA, позволяет фрейморку достич выской производительности. Фреймворк имеет простой Lua-интерфейс, что позволяет легко подключать его к стороннему программному обеспечению.
\subsection{Основыне характеристики Torch7}
\par Структура фрейворка имеет три основных преимущества:
\begin{itemize}
\item она облегчает разработку численных методов;
\item фреймворк легко расширяем (включая использование сторонних библиотек);
\item высокая скорость работы фреймворка.
\end{itemize}
\par Второе преимущества достигается за счет выбранных разработчиками технологий. Скрипровый (интерпретируемый) язык с хорошим API-интерфейсом для C обеспечивает фреймворку гибкост в разработке и не накладывает ограничения на его расширяемость. Так как, язык высокого уровня делает процесс разработки программы более простым и понятным, чем язык низкого уровня. К тому же, интерпретируемость позволяет быстро и легко реализовывать различные идеи в интерактивном режиме. Хороший API-интерфейс сохраняет функциональные возможности из разных библиотек, так как становиться прослойкой между универсальной структурой на языке Lua и различными структурами используемых библиотек на языке C.
\par Высокая скорость работы достигается благодаря компилятору JIT (Just In Time). На данный момет Lua является самым быстрым интерпритируемым языком. Lua разрабатывался для легкого внедрения в приложения, написанные на C. Поэтому представляет большое C-API на основе виртуального стека, для передачи значений между Lua и C. Это унифицирует интерфейс для C/C++ и делает обертывание библиотек тривиальным [4].
\par Lua предназначен для использования в качестве мощного, легкого скриптового языка обладающими всеми необходимыми выразительными средствами. Он реализован как библиотека, которая написана на чистом C (точнее на подмножестве ANSI C и C++). Lua сочетает простой процедурный синтаксис с мощными конструкциями описания данных на основе ассоциативных массивов и расширяемой семантики. Lua динамически типизируется, выполняется путем интерпретации байт-кода для виртуальной машины на основе регистров и имеет автоматическое управление памятью с инкрементной сборкой мусора, что делает его идеальным для настройки, написания сценариев и быстрого прототипирования [5].
\par Lua предлагает хорошую поддержку объектно-ориентированного программирования, функционального программирования и программирования, управляемого данными. Основным типом Lua является таблица, которая реализует ассоциативные массивы очень эффективным способом. Ассоциативный массив --- это массив, который может индексироваться не только числами, но и любыми другими типами данных язка. Таблицы не имеют фиксированного размера, они динамически изменяемы и могут использоваться как "виртуальные таблицы" над другой таблицей, что позволяет имитировать парадигмы объектно-ориентированного программирования. Таблицы являются единственным, но очень мощным механизмом структурирования данных в Lua. Torch7 использует таблицы для простого, равномерного и эффективного представления обычных массивов, таблиц символов, кортежей, очередей и других структур данных. Lua также использует таблицы для представления пакетов.
\par Lua и Python очень схожи как по структурированию данных, так и по стилю программирования. Если говорить о популярности в сообществе, то Python опережает Lua из-за огромного количества поставляемых библиотек. Однако разработчики выбрали Lua по ряду других причин, которые, в виду специфики фреймворка, являются ключевыми. Во-первых, интеграция Lua с C очень проста. За несколько часов любая библиотека на C или C++ может стать библиотекой Lua. Во-вторых, Lua предоставляет эффективные возможности встраивания. Что бы преобразовать прототип в финальный продукт требуется не много дополнительной работы. В-третьих, Lua обладает высокой производительностью благорадя интерпритатору LuaJIT, который выдает производительность на уровне C. Еще одним преимуществом Lua является переносимость. Lua написан на чистом ANSI C, его можно скомпилировать для любых устройств (сотовые телефоны, встроенные процессоры в FPGA, процессоры DSP и др.).
\subsection{Структуры используемых типов данных}
\par Ключевой сущностью в Torch7 является класс Tensor, поставляемый автономной С-библиотекой Tensor. Данный класс расширяет базовый набор типов Lua, чтобы реализовать эффективную работу с многомерными массивами. Большинство пакетов Torch7 или сторонних пакетов, зависящих от Torch7, реализуют собственный класс Tensor для представления сигналов, изображений, видео и других объектов, что упрощает интегрирование различных библиотек. Библиотека Torch Tensor предоставляет множество классических операций (включая операции линейной алгебры), которые реализованны и оптимизированны на С, используются SSE интсрукции для Intel платформ. Опцианально можно использовать высокопроизовидельные реализации операций линейной алгебры в библиотеке BLAS. Так же данная библиотека поддерживает инструкции OpenMP и вычисления на CUDA GPU.
\subsection{Пакеты Torch7}
\par На данный момент Torch7 имеет 7 основных пакетов:
\begin{itemize}
\item torch: основной пакет Torch7. Обеспечивает фреймворк классом Tensor, облегчает сереализацию и другие базовые функции;
\item lap и plot: представляют стандартные функции для создания, преобрзования и визуализации объектов Tensor. Пример работы показан на рисунке ~\ref{torch_plots}
\begin{figure}
    \includegraphics[width=\linewidth]{torch_plots.png}
    \caption{Графики, полученные с помощую пакета plot фреймворка Torch7. Слева: простые синусоидальные функций. В центре: Поверхность, хранящаяся в 2D Tensor. Справа: Матричный график, построенный с использованием карты тепла}
	\label{torch_plots}
\end{figure}
\item qt: предаставляет интерфейс работы Torch7 с Qt. Реализует конвертацию Tensor в QImage и наоборот. Отлично подходит для быстрого создания интерактивных демонстраци с кроссплатформенным графическим интерфейсом.
\item nn: предоставляет набод стандартных модулей для создания нейронной сети. В пакет так же входит набор контейнерных модулей, которые можно использовать для определения произвольно направленных графов. Явное описание графа позволяет избежать сложности с анализатором графов или любого другого компилятора промежуточного уровня.
\par На практике нейронная сеть представляет собой последовательные графы, либо графы с шаблонными витвлениями и рекурсиями. На рисунке ~\ref{nn_create} показано создание многослойного перцептрона, используя пакет nn.
\begin{figure}
    \includegraphics[width=0.3\paperheight]{nn_create.png}
    \caption{Создание многослойного перцептрона, используя пакет nn}
	\label{nn_create}
\end{figure}
\par Каждый модуль или контейнер имеет стандартные функции для вычисления выходного состояния, обратного распространения производных входов и внутренних параметров. Для нейронной сети, приведенной на рисунке ~\ref{nn_create}, вызов этих функций показан на рисунке ~\ref{nn_forward}.
\begin{figure}
    \includegraphics[width=1.2\linewidth]{nn_forward.png}
    \caption{Вычисление выходного состояния, обратного распространения производных входов и внутренних параметров}
	\label{nn_forward}
\end{figure}
\item image: пакет обработки изображений. Данный пакет поставляет стандартные функции работы с изображениями (сохранение, загрузка, маштабирование, вращение, конвертация цветовых пространст, свертка и др.).
\item optim: компактный пакет, который обеспечивает фреймворк методами оптимизации. В него входят реалиция наклонного спуска, сопряженного градиента и алгоритма Бройдена --- Флетчера --- Гольдфарба --- Шанно (BFGS).
\item unsup: содержит алгоритмы обучения без учителя, такие как K-means, разреженное кодирование и автокодеры.
\end{itemize}
\par В дополнение к основным доступен постоянно растущий список сторонних пакетов. К примеру, mattorch, который обеспечивает двухсторонний интерфейс между матричным форматом Matlab и форматом Tensor или parallel, который предоставляет функции разветвления и исполнения Lua-кода на локальных или удаленных машинах, используя механизм сериализации Torch7. Этот список постоянно растет, поскольку Lua упращаяет интерфейс любой билиотеки C.
\section{Darknet}
Darknet является фреймворком машинного обучения с открытым исходным кодом, написанным на C и CUDA. Он прост в установке и поддерживает вычисления как на центральном процессоре, так и на графическом.
\subsection{Основные характерискики Darknet}
\par Darknet один из немногих фреймворков машинного обучения, который не имеет обязательных зависимостей. Что позволяет быстро разворачивать его на встраиваемых системах. На ряду с встроенным функционалом, Darket поставляется с двумя опциональными зависимостями:
\begin{itemize}
\item OpenCV --- для предоставление более широкого спектра поддерживаемых форматов изображений;
\item CUDA --- для вычислений на GPU. 
\end{itemize}
\par Обе не являются обязательными для установки фреймворка.
\par Еще одним важным преимуществом фреймворка является независимость от архитектуры системы. Darknet полностью написан на C, что делает его универсальным, а его интеграцию в встраиваемы системы или в специализированное оборудование простой и понятной. 
\par В оригинальном виде фреймворк, поставляемый разработчиками, представляет консольное приложения для работы с нейронными сетями. С помощью него можно проектрировать, обучать, тестировать нейронные сети типовых топологий. В список функций так же входит визуализация модели классификации и обучение реккурентных моделей. Однако, конфигурация файлов исходных кодов спроектирована специально для предоставляения возможности компиляции необходимых модулей в библиотеку. Поэтому фреймворк можно встраивать как нативную библиотеку в любой пользовательский проект.
\par Важной особенностью фреймворка является оптимизация работы с памятью и с вычислительными ресурсами. Это позволяет работать с визуальными задачами даже на устройствах с ограничеммыми ресурсами памяти. Darnket имеет две эффективные реализации сверточных нейронных сетей: сети с бинарными весами и XNOR-сети. В сетях с бинарными весами фильтры аппроксимируются двоичными значениями, что приводит к экономи памяти в 32 раза. В XNOR-сетяк как фильтры, так и входные данные для сверточных слоев являются двоичными. XNOR-сети реализуют свертки, используя в основном бинарные опирации. Это приводит к ускорению сверточныз операций в 58 раз и экономии памяти в 32 раза. Данная оптимизация позволяет запускать современный нейронные сети на центральных процессорах в режиме реального времени. Если говорить о точности работы, то классификация модели AlexNet на 2.9 \% меньше у сети с бинарными весами оп сравнению с оригинальной реализацией. Метод используемый в сетях с бинарными весами и XNOR-сетех превосходит новейшие сетевые методы бинаризации (BinaryConnect и BinaryNets) на 16 \% (тест профодился на классификаю, используя модель ImageNet)[6].
\subsection{Используемые структуры данных}
\par Ключивыми типами данных в Darknet явлюются структуры network и layer. Структура layer представляет собой объект для параметров слоя сети. Данная структура имеет общий интерфейс для всях типов слоев, поэтому обладает большим набором параметров. Для расчета выходов и градиента слоев, структура предоставляет два указателя на функции forward и backward соответственно. Реализации данных функций находятся в файлах исходных кодов у каждого типа слоя. Такая модульная структура позволяет быстро добавлять новый типы слоев и компактно реализовывать операции работы с нейронной сетью. В целом, слои представляют двунаправленный связанный список, что соответствует логике работы с нейронными сетями. 
\par Структура network определяет абстрактную модель для хранения внутренних параметров нейронной сети. Как и Caffe, Darknet разделяет представление и реализацию. Это реализуется разделением данных модели на конфигурационный файл, в котором определены внутренние параметры, и на бинарный файл с весами модели. Конфигурационный файл имеет строковый формат и представляет собой описание параметров нейронной сети, параметров обучения, параметров слоев и их последовательность. Формат конфигурационного файла представлен на рисунке ~\ref{network_cfg}
\begin{figure}
    \includegraphics[width=0.4\paperheight]{network_cfg.png}
    \caption{Формат конфигурационного файла нейронной сети}
	\label{network_cfg}
\end{figure}
\par Основной структурой данных в фреймворке является динамический массив. Веса, изображения, строковые таблицы храняться в одномерном массие, который обернут в структуру соответствующего типа данных. Данный подход позволяет сократить издержки работы с памятью.
\section{Сравнение фреймворков машинного обучения}
Для использования сверточной нейронной сети на системе с ограниченными вычислительными ресурсами и ресурсами памяти необходимо, что бы фреймворк, поставляющий данные функции удовлетворял следующим условиям:
\begin{itemize}
\item высокопроизворительные вычисления;
\item оптимизированная работа с памятью;
\item минимальное число зависимостей.
\end{itemize}
\par Рассмотринные выше фреймворки, используя различные терхнологии и алгоритмы, обеспечивают высокую производительность своих реализаций. Caffe использует библиотеку BLAS (ATLAS, Intel MKL, OpenBLAS) для векторных и матричных вычислений, Lua в совокупности с технологиями SSE, OpenMP позволяют Torch показывать высокую скорость работы, бинаризация ядер в Darknet, позволяет использовать быстрые бинарные операции для расчетов.
\par Если говорить о оптимизации работы с памятью, то аппроксимация фильтров и входов в Darknet позволяет значительно уменьшить объем выделяемой памяти. На рисунке ~\ref{binary_conv} сравнение бинарной свертки и свертки с двойной точностью.
\begin{figure}
    \includegraphics[width=0.55\paperheight]{binary_conv.png}
    \caption{Эффективность использования памяти и вычислений. a -- выделяемая память для весов различных архитектур, b --- ускорение в зависимости от числа каналов, c --- ускорение в зависимости от размера фильтра}
	\label{binary_conv}
\end{figure}
\par Caffe и Torch имеют достаточно большое количество зависимостей. Это объясняется желанием максимально ускорить процессы обучения и прохода нейронных сетей, однако наклабывает ограничения на срециализированное оборудование и оборудование с ограниченными запасами физической памяти.
\par Суммировав все показатели, можно сделать вывод, что Darknet является лучшим вариантов для разворачивания на мобильном пк C.H.I.P.

\chapter{Используемые алгоритмы и модели}
\section{Теоретические основы нейронных сетей}
\subsection{Нейронные сети: основные положения}
\par Основой любой нейронной сети являются однотипные, простые элементы, которые представляют собой упрощенную модель нейронов мозга. Далее по тексту термин “нейрон” используется для определения ячейки нейронной сети --- искусственного нейрона. В соответствии с клетками головного мозга, которые могут быть возбужденными или заторможенными, нейрон характеризуется состоянием в момент прохода нейронной сети. Каждый нейрон обладает набором синапсов и одним аксоном. Синапсы являются однонаправленными связями, которые связывают конкретный нейрон с выходами группы других нейронов. В свою очередь, аксон передает сигнал нейрона на синапсы нейронов, расположенных на следующем слое. На рисунке ~\ref{neuron} представлен общий вид нейрона. Каждый синапс описывается величиной синаптической связи, иными словами, синапсы характеризуются весом $w_i$, который является аналогом электрической проводимости в клетках мозга.
\begin{figure}
    \includegraphics[width=0.6\linewidth]{neuron.png}
    \caption{Исскуственный нейрон}
	\label{neuron}
\end{figure}
\par Состояние нейрона в момент прохода нейронной сети определяется как взвешенная сумма его входов:

\[
\ \hspace*{66mm} s = \sum_{i=1}^n x_i w_i \hspace*{62mm} (1)
\] 
\par Выходом нейрона является функция от его состояния:
\[
\ \hspace*{66mm} y = f(x) \hspace*{66mm} (2)
\] 
\par Функция f должна обладать свойством нелинейности. Это необходимо для создания многослойных нейронных сетей. Если в НС используется пороговая функция, то смысла в ее многослойности нет, так как такая сеть эквивалентна сети с одним скрытым слоем и с весовой матрицей единственного слоя. 
\par Нелинейная функция f именуется активационной функцией нейрона. На данный момент существует огромное количество видов активационных функций. На рисунке ~\ref{activation_func} показаны некоторые из них.
\begin{figure}
    \includegraphics[width=0.6\linewidth]{activation_func.png}
    \caption{а) функция единичного скачка; б)линейный порог (гистерезис); в) сигмоид – гиперболический тангенс; г) сигмоид – формула (3)}
	\label{activation_func}
\end{figure}
\par Одной из самых первых используемых активационных функций является логистическая функция или сигмоид (функция имеет S-образный вид): 
\[
\ \hspace*{66mm} f(x) = \frac{1}{1 + e^{- \alpha x}}\ \hspace*{56mm} (2)
\] 
\par Чем меньше параметр $\alpha$, тем функция становится более пологой. В пределе при $\alpha=0$ сигмоид вырождается в горизонтальную прямую в значении 0.5. Если увеличивать a, то сигмоид преобразуется в функцию единичного скачка в точке $x=0$. Значение данной активационной функции лежит в интервале [0, 1]. Популярность функции обеспечивает простота ее производной, которая используется при обучении НС. 
\[
\ \hspace*{66mm} f'(x) = \alpha f(x) (1 - f(x)) \ \hspace*{56mm} (2)
\] 
\par Логистическая функция дифференцируема на всей оси абсцисс. Это свойство используется в некоторых алгоритмах обучения. Также, сигмоид усиливает слабые сигналы лучше, чем большие, это позволяет избежать перенасыщения от больших сигналов, так как области определения больших сигналов соответствуют пологому наклону функции.   
\par Если говорить про обработку сигналов НС, то, зачастую, они обрабатываются параллельно. Это достигается с помощью объединения большой группы нейронов в слои и соединения определенным образом нейроны разных слоев. Существуют конфигурации, где нейроны одного слоя соединены между собой. Данная конфигурация обрабатывается послойно.
\par На рис ~\ref{perceptron} изображен простейшая конфигурация нейронной сети --- трехнейронный перцептрон. Пусть нейронной этой НС используют активационную функцию в виде скачка. 
\begin{figure}
    \includegraphics[width=0.4\linewidth]{perceptron.png}
    \caption{Однослойный перцептрон}
	\label{perceptron}
\end{figure}
\par На n входов поступают некоторые сигналы, которые распространяются на три нейрона, образующие скрытый слой НС. Каждый нейрон выдает сигнал:
\[
\ \hspace*{66mm} y_j = f\biggl[\sum_{i=1}^n x_i w_{ij} \biggr], j=1 \dots 3  \ \hspace*{56mm} (2)
\] 
\par Из весовых коэффициентов можно составить матрицу W, в которой wij -- вес i-того входного сигнала в j-том нейроне. Тогда, процесс прохода НС описывается в матричной форме следующим образом:
\[
\ \hspace*{66mm} Y=F(XW)  \ \hspace*{56mm} (2)
\] 
где X --- вектор входных сигналов, Y --- вектор выходных сигналов, F(XW) --- активационная функция, выполняющаяся над каждым элементом вектора XW.
\par Теоретически количество слоев (глубина) и количество нейронов в них (высота), используемых в НС, не ограниченно, но фактически ограничения накладывают вычислительные мощности устройства, на котором выполняется обработка НС. Но чем сложнее НС, тем масштабнее задачи она может решить. 
\par Структура НС зависит от сложности задачи. Оптимальные конфигурации для некоторых типов задачи уже реализованны и описаны, например в [8],[9],[10]. Если же задача не является типовой, то разработчик самостоятельно генерирует модель, в зависимости от сложности задачи, размера обучающей выборки и вычислительных ресурсов. При этом необходимо учитывать основопологающие принципы: качество модели напрямую зависит от количества нейронов сети, плотности связей между ними и количеством слоев; сложность алгоритмов функционирование сети (например, введение нескольких типов синапрос, использование непороговых активационных функций) влиет на производительность НС. Задача поиска оптимальной конфигурации для той или иной задачи является отдельным направлением нейрокомпьютерной науки. Синтез нейронной сети напряму зависит от типа решаемой задачи, поэтому список подробных рекомендаций составить затруднительно. В большинстве случаев оптимальный вариант выбирается эмпирическим методом.
\par Очевидно, что функционирование нейронной сети напрямую зависит от величины синаптических связей между нейронами. Поэтому, после нахождения конфигурации нейронной сети, разработчик должен найти оптимальные значения всех переменных весовых коэффициентов (некоторые веса могут быть постоянными).
\par Описанный процесс называется обучений нейронной сети, он является ключевым при сознании НС. От того, насколько хорошо он будет выполнен, зависит качество решений поставленных задачь перед нейронных сетей. На этапе обучение кроме качества поиска весов важное место занимает такой параметр как время обучения. Эти два параметра обратно пропорциональны: чем лучше подобраны веса, тем больше затрачено времени на обучение.
\par Сущетвует два варианта обучения: с учителем и без него. В первом случае, при обучении предоставляются как входны сигналы, так и желаемые выходные. Далее обучение представляет собой алгоритм подгонки весов, таким образом, что бы желаемы выходные сигнали совпадали с выходными сигналами НС. Во втором случае, выходы генерируется нейронной сетью, а при обучении учитываются только входные и производные от них сигналы.
\par Существующие алгоритмы машинного обучения делятся на два типа: детерминистские и стохастические. В первом случае подбор оптимальных весом представляет собой четкую последовательность, во втором --- подчинен некоторому случайному процессу. 
\par Необходимо сказать, что среди классификаций НС важное место занимают бинарные и аналоговые сети. Первые используют двоичные сигналы, в результате чего выход каждого из нейров принимает одно из двух значений: логический ноль ("заторможенное" состояние) или логическая единица ("возбужденное" состояние). К такой классификации относится перцептрон, описанные выше. Его активационная функция является пороговой, значение которой либо 0 либо 1. В аналоговых сетях выходное значение нейронов может быть непрерывным, это реализуется использованием в качестве активационных непрерывные функции, например сигмоид. 
\par Еще одна классификация разделяет НС на синхронные и асинхронные[9]. Первый случай предпологает изменение состояния только одного нейрона в каждый момент времени. Во втором случае изменние произходит одновременно у группы нейронов, как правило, у всего слоя. Ход времени в НС представлен последовательным выполненим однотипных действий над нейронами. В данной главе будут рассмотрены только синхронные НС.
\par Обычно, сети классифицируют по числу слоев. На ~\ref{perceptron2} показана НС полученная добавлением еще одного слоя, состоящего из двух нейронов, в НС, изображенную на ~\ref{perceptron}. Слои, которые не являются входными и выходными, называются скрытыми. 
\begin{figure}
    \includegraphics[width=0.6\linewidth]{perceptron2.png}
    \caption{Двухслойный перцептрон}
	\label{perceptron2}
\end{figure} 
\par Бывают случае, когда нелинейность используется еще и в синаптических связях. Большинство современных сетей используют формулу (1) для вычисления значения нейронна, однако, для эффективного решения некоторых задач используется другая запись, например:
\[
\ \hspace*{66mm} s = \sum_{i=1}^n x_{i}^{2} w_i  \ \hspace*{56mm} (2)
\] 
\par или даже
\[
\ \hspace*{66mm} s = \sum_{i=1}^n x_{i}^{2} x_{((i+1)\bmod n)} w_i  \ \hspace*{56mm} (2)
\] 
\par Главное, что бы разработчик понимал, какие цели он преследует при наделении нейрона подобной связью и какие ограничения на нейрон накладываются. Введение таких нелинейность увеличивает вычислительную мощность НС, другими словами, позволяет уменьшить число нейронов и связей без потери качества работы[8]. 
\par При обучении НС учитывается не только время процесса и качество обучения. Помимо этих параметров необходимо подобрать пороговое значение T. Из рисунка ~\ref{activation_func} видно, что, в общем случает, T может принимать произвольное значение. То же самое относиться и к центральной точке сигмоиды, положение которой изменяется по оси X влево или вправо. В общем случии каждая активационная функция имеет параметр, который необходимо подобрать при обучении. В связи с этим формула (1) должа выглядеть следующим образом:
\[
\ \hspace*{66mm} s = \sum_{i=1}^n x_{i} w_i-T \ \hspace*{56mm} (11)
\] 
\par Что бы добавить данное смещение, необходимо добавить еще один вход, которые меет синаптическую связь со всеми нейронами слоя. На этот вход всегда "возбужденный" сигнал. Присвоим такому входу номер 0. Тогда
\[
\ \hspace*{66mm} s = \sum_{i=1}^n x_{i} w_i-T \ \hspace*{56mm} (11)
\] 
\par где $w_0=-T, x_0=1.$
\par Из чего следует, что отличие формулы (1) от формулы (12) в способе нумерации входов.
\par Все задачи, которая решает НС можно свести к классификации. Грубо говоря задача НС обпределить к какому классу принадлежит группа входных сигналов, находящихся в n-мерном пространстве. С математической точки зрения процесс представляет собой разбиение гиперпространство гиперплоскостями на области.
\par К каждой области принадлежит отдельный класс. Максимальное число классов для НС перцептронного типа не превышает $2^m$, где m --- число выходов сети. Однако существует ограничение на формы гиперплоскостей, иначе говоря, не все нейронные сети могут разделить n-мерное пространство на необходимое количество классов.
\par Например, однослойный перцептрон, с одним нейроном, изображенный на рисунке ~\ref{single_neuron} не способен разделить двумерное пространство на две
полуплоскости так, что бы классифисировать входные сигналы на классы A и B (см. ~\ref{a_and_b}).
\begin{figure}
    \includegraphics[width=0.6\linewidth]{single_neuron.png}
    \caption{Однонейронный перцептрон}
	\label{single_neuron}
\end{figure}
\begin{longtable}{|c|c|c|c|}
    \caption{Классификация XOR}\\ \hline
    \label{a_and_b} 
    x1 & x2 & A & B \\ \hline
    0  & 0  & • & \hspace*{1mm}  \\ \hline
    0  & 1  & \hspace*{1mm} &  • \\ \hline
    1  & 0  & \hspace*{1mm} &  • \\ \hline
    1  & 1  & • & \hspace*{1mm} \\ \hline
\end{longtable}

\par Сеть, представленная на рисунке ~\ref{single_neuron} описывает следующие уравнение:
\[
\ \hspace*{66mm} x_1 w_1 + x_2 w_2=T \ \hspace*{56mm} (11)
\] 
\par Данное уравнение является прямой, которая не способна разделить плоскольсть таким образом, что бы группа входных сигналов $x_1, x_2$ пренадлежали необходимым классам. На рисунке ~\ref{line_class} показана работа НС.
\begin{figure}
    \includegraphics[width=0.6\linewidth]{line_class.png}
    \caption{Визуальное представление работы НС с рисунка ~\ref{single_neuron}}
	\label{line_class}
\end{figure}
\par Таблица ~\ref{a_and_b} является таблицей истинности для логической функции исключающего ИЛИ. Невозможность реализовать данную функцию, используя однонейронный перцептрон, получила название проблемы исключающего ИЛИ.
\par Задачи, которые не решаются однослойной сеть, называются линейно неразделимыми[8]. Для решения таких задачь используются нейронные сети сбольшим количеством скрытый слоев. Однако, и в таких случаях корректное разделение на классы не гарантируется. Как было сказано раньше, конфигурация НС это итеративный эмпирический процесс.
\par После обзора теоритических основ нейроной сети, можно более подробродно рассмотреть алгоритм обучения с учителем, на основу взят перцептрон, изображенный на рисунке ~\ref{perceptron}.
\par Алгоритм выглядит следующим образом[8]:
\par 1) Проинициализировать весовые коэффициэны случайными значениями.
\par 2) Подать на вход вектор вхожных сигналов, вычислить выходные сигналы.
\par 3) Если выход совпарает с желаемыми значениями, перейти на шаг 4.
Иначе вычислить разницу желаемым и полученным значение НС:
\[
\ \hspace*{66mm} \delta=Y_I-Y \ \hspace*{56mm} (11)
\] 
\par Изменить веса в соответствии с формулой:
\[
\ \hspace*{66mm} w_{ij}(t+1)=w_{ij}(t)+\nu        \delta x_i \ \hspace*{56mm} (11)
\]
гдe $t$ и $t+1$ --- номера текущей и следующей итерации; $\nu$ --- коэффициент скорости обучения; $i$ --- номер входа; $j$ --- номер нейрона в слое.
\par Веса будут увеличины, если $Y_I>I$, тем самым ошибка уменьшится. В обратном случае они будут уменьшены, и Y соответственно тоже уменьшится, приближаясь к $Y_I$.
\par 4) Повтор шага 2, пока не будет достигнута желаемая точность.
\par На втором шаге на вход НС подаются все входные вектора из обучающей выборки в случайном порядке. Число итераций зависит от сложности задачи и конфигурации нейронной сети. Определить точное количество итераций, необходимых для корректного обучения определить невозвожно. 

\subsection{Сверточные нейронные сети}
\subsection{Обнаружение объектов с применем подхода YOLO}

\chapter{Проектирование системы}
\section{Оптимизация работы с памятью}
\subsection{Бинаризация весов}
\subsection{Оптимизация работы со слоями}

\newpage
\starchapter{Заключение}
\newpage
\begin{thebibliography}{1}
    \bibitem{1} https://arxiv.org/pdf/1408.5093.pdf
    \bibitem{2} J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang, 
E. Tzeng, and T. Darrell. Decaf: A deep convolutional
activation feature for generic visual recognition. In ICML,
2014
  	\bibitem{3}  A. Krizhevsky, I. Sutskever, and G. Hinton. ImageNet
classification with deep convolutional neural networks. In
NIPS, 2012
	\bibitem{4} http://ronan.collobert.com/pub/matos/2011\_torch7\_nipsw.pdf
	\bibitem{5} http://www.lua.ru/doc/1.html
	\bibitem{6} https://pjreddie.com/media/files/papers/xnor.pdf
	\bibitem{7} http://www.shestopaloff.ca/kyriako/Russian/Artificial\_Intelligence/Some\_publications/Korotky\_Neuron\_network\_Lectures.pdf
	\bibitem{7} 1. Е. Монахова, "Нейрохирурги" с Ордынки, PC Week/RE, №9, 1995.
	\bibitem{8}2. Ф.Уоссермен, Нейрокомпьютерная техника, М.,Мир, 1992.
	\bibitem{9}3. Итоги науки и техники: физические и математические модели нейронных сетей, том
1, М., изд. ВИНИТИ, 1990.

 
\end{thebibliography}


\appendixdocument{Техническое задание}
\end{document}
